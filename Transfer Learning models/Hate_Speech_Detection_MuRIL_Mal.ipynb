{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltGf_wdgxfjU",
    "outputId": "1b9db12d-91ed-40d2-93d0-825435bca332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpCpBVKNf0Xr"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HemPFJK8xlu_",
    "outputId": "d1db8bfa-583b-4977-913e-259a8e232019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.5.0\n",
      "Hub version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from sklearn import preprocessing\n",
    "from bert import bert_tokenization\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8_Ufz-lgBIR"
   },
   "source": [
    "## Reading Datasets and droping nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "k87Dyxkaxk9P"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('/content/drive/MyDrive/Datasets/Malayalam__hasoc_train.xlsx',names=[\"ID\",\"Tweets\",\"Labels\"])\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_val = pd.read_csv(\"/content/drive/MyDrive/Datasets/malayalam_hasoc_dev.tsv\", sep=\"\\t\",names=[\"ID\",\"Tweets\",\"Labels\"])\n",
    "df_val.dropna(inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IfeKjZqoEet9"
   },
   "outputs": [],
   "source": [
    "df_train.Tweets =  df_train.Tweets.apply(preprocessing)\n",
    "df_val.Tweets =  df_val.Tweets.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avem0vrlh_SK"
   },
   "source": [
    "## Mapping the labels correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nUMAC7huam2L"
   },
   "outputs": [],
   "source": [
    "df_train.Labels = df_train.Labels.map({'not': 'NOT', 'OFf': 'OFF','NOT': 'NOT', 'OFF': 'OFF'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iImgfAT5ghLm"
   },
   "source": [
    "## Label Encoding to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0jMoaMsxlBf",
    "outputId": "9e93edb4-6b77-4264-b335-17c4b5b39d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique labels 2\n"
     ]
    }
   ],
   "source": [
    "unique_labels = list(np.unique(df_train[\"Labels\"]))\n",
    "\n",
    "train_x = df_train[\"Tweets\"].values\n",
    "train_y = df_train[\"Labels\"].values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "train_y = tf.keras.utils.to_categorical(train_y, num_classes=len(unique_labels), dtype='float32')\n",
    "\n",
    "val_x = df_val[\"Tweets\"].values\n",
    "val_y = df_val[\"Labels\"].values\n",
    "\n",
    "val_y = le.fit_transform(val_y)\n",
    "val_y = tf.keras.utils.to_categorical(val_y, num_classes=len(unique_labels), dtype='float32')\n",
    "\n",
    "\n",
    "print(\"number of unique labels\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPARLi5tg96h"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3E59zL9VxlKY"
   },
   "outputs": [],
   "source": [
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "# Function to create attention masks\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "# Function to create segment ids\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "# Function to create input_ids, attention_masks, segment_ids for sample\n",
    "def create_single_input(sentence,MAX_LEN, MAX_SEQ_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    "\n",
    "  return ids,masks,segments\n",
    "\n",
    "def create_input_array(sentences, MAX_SEQ_LEN):\n",
    "\n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    "\n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2, MAX_SEQ_LEN)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    "\n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrB7PsnAxlMf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fCOVNq8hAJV"
   },
   "source": [
    "## Downloading the MuRIL model from TFHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "shTcA17AxlO6"
   },
   "outputs": [],
   "source": [
    "muril_layer = hub.KerasLayer(\"https://tfhub.dev/google/MuRIL/1\", trainable=True)\n",
    "\n",
    "# Create tokenizer\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CITNL4R-ygL8",
    "outputId": "93f860a4-ca6a-447b-feea-4021cb39fd58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3999/3999 [00:01<00:00, 3091.57it/s]\n",
      "100%|██████████| 951/951 [00:00<00:00, 3050.93it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 120\n",
    "train_x = create_input_array(train_x, max_seq_len)\n",
    "val_x = create_input_array(val_x, max_seq_len)\n",
    "# test_x = create_input_array(test_x, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGmP4G1LhH1v"
   },
   "source": [
    "## Defining the F1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2AU7HnwiygOS"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bJkmnDkF82cI",
    "outputId": "a7cf1470-c58b-4054-ebff-2a2dc87af1dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Downloading tf_models_official-2.5.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▏                               | 10 kB 37.1 MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 20 kB 24.4 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 30 kB 17.5 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 40 kB 15.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 51 kB 8.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 61 kB 7.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 71 kB 8.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 81 kB 9.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 92 kB 9.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 102 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 112 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 122 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 133 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 143 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 153 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 163 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 174 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 184 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 194 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 204 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 215 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 225 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 235 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 245 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 256 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 266 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 276 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 286 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 296 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 307 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 317 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 327 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 337 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 348 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 358 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 368 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 378 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 389 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 399 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 409 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 419 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 430 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 440 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 450 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 460 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 471 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 481 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 491 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 501 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 512 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 522 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 532 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 542 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 552 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 563 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 573 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 583 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 593 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 604 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 614 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 624 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 634 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 645 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 655 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 665 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 675 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 686 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 696 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 706 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 716 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 727 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 737 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 747 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 757 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 768 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 778 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 788 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 798 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 808 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 819 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 829 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 839 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 849 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 860 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 870 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 880 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 890 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 901 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 911 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 921 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 931 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 942 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 952 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 962 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 972 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 983 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 993 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 1.0 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.0 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.0 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 1.0 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 1.0 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 1.1 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 1.2 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 1.3 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.4 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.5 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.1.5)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.2)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.0.1)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 24.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.5.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.4.1)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 10.4 MB/s \n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.1.96)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (7.1.2)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 46 kB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 65.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.24)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.15.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 12.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.12.0)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 69.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.8)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 66.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.34.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.26.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2018.9)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (21.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (57.2.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.17.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2021.5.30)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.62.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (0.12.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.1.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.34.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (0.37.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (3.3.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official) (0.2.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (0.4.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (4.6.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (2019.12.20)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (0.8.9)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.2.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (21.2.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (1.2.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.4)\n",
      "Building wheels for collected packages: py-cpuinfo, seqeval\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=ee69abffc7ecb71c96820c0370826d1727f13306ffaffad679d8969993a5535e\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=3bb3d48baaffcfa3197bdecc33e0326287f762ef43d2ebb32b70144d3e2995c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built py-cpuinfo seqeval\n",
      "Installing collected packages: portalocker, colorama, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, tf-models-official\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed colorama-0.4.4 opencv-python-headless-4.5.3.56 portalocker-2.3.0 py-cpuinfo-8.0.0 pyyaml-5.4.1 sacrebleu-2.0.0 seqeval-1.2.2 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tf-models-official-2.5.1 tf-slim-1.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "yaml"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tf-models-official"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qOjEl4rg2tB"
   },
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7Ka_06a4ygaQ"
   },
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(120,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(120,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(120,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "  \n",
    "outputs = muril_layer(dict(input_word_ids = input_word_ids, input_mask = input_mask, input_type_ids = segment_ids))\n",
    "x = tf.keras.layers.Dropout(0.2)(outputs[\"pooled_output\"]) # take pooled output layer\n",
    "final_output = tf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "      inputs=[input_word_ids, input_mask, segment_ids], outputs=final_output)\n",
    "\n",
    "    \n",
    "#   optimizer = \n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  metrics=['accuracy',f1_m])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmaGvzuLygcZ",
    "outputId": "2af5733c-7542-4d6f-b451-3da4b8b25617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_mask (InputLayer)         [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_word_ids (InputLayer)     [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      {'default': (None, 7 237556225   input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "                                                                 input_word_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           keras_layer_1[0][13]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_output (Dense)            (None, 2)            1538        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 237,557,763\n",
      "Trainable params: 237,557,762\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNxTXw-6hiI_"
   },
   "source": [
    "## Making checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JoiYEhtPfQkw"
   },
   "outputs": [],
   "source": [
    "metric = 'val_f1_m'\n",
    "model_save_path='/content/m1'\n",
    "import keras\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor=metric,mode='max',save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bm_nBmxAygew",
    "outputId": "fe17eb1c-b8c3-4546-816b-4948ee4c1c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "80/80 [==============================] - 120s 1s/step - loss: 0.6928 - accuracy: 0.5204 - f1_m: 0.5097 - val_loss: 0.6906 - val_accuracy: 0.6372 - val_f1_m: 0.6450\n",
      "Epoch 2/15\n",
      "80/80 [==============================] - 106s 1s/step - loss: 0.6837 - accuracy: 0.6082 - f1_m: 0.6061 - val_loss: 0.6509 - val_accuracy: 0.7213 - val_f1_m: 0.7368\n",
      "Epoch 3/15\n",
      "80/80 [==============================] - 106s 1s/step - loss: 0.6461 - accuracy: 0.6724 - f1_m: 0.6719 - val_loss: 0.5992 - val_accuracy: 0.7287 - val_f1_m: 0.7414\n",
      "Epoch 4/15\n",
      "80/80 [==============================] - 107s 1s/step - loss: 0.5910 - accuracy: 0.7287 - f1_m: 0.7290 - val_loss: 0.5616 - val_accuracy: 0.7476 - val_f1_m: 0.7606\n",
      "Epoch 5/15\n",
      "80/80 [==============================] - 107s 1s/step - loss: 0.5277 - accuracy: 0.7727 - f1_m: 0.7726 - val_loss: 0.5298 - val_accuracy: 0.7603 - val_f1_m: 0.7716\n",
      "Epoch 6/15\n",
      "80/80 [==============================] - 107s 1s/step - loss: 0.4607 - accuracy: 0.8165 - f1_m: 0.8167 - val_loss: 0.5173 - val_accuracy: 0.7666 - val_f1_m: 0.7780\n",
      "Epoch 7/15\n",
      "80/80 [==============================] - 107s 1s/step - loss: 0.4134 - accuracy: 0.8402 - f1_m: 0.8401 - val_loss: 0.5295 - val_accuracy: 0.7529 - val_f1_m: 0.7646\n",
      "Epoch 8/15\n",
      "80/80 [==============================] - 109s 1s/step - loss: 0.3598 - accuracy: 0.8682 - f1_m: 0.8680 - val_loss: 0.5471 - val_accuracy: 0.7539 - val_f1_m: 0.7660\n",
      "Epoch 9/15\n",
      "80/80 [==============================] - 110s 1s/step - loss: 0.3132 - accuracy: 0.8912 - f1_m: 0.8912 - val_loss: 0.5820 - val_accuracy: 0.7476 - val_f1_m: 0.7600\n",
      "Epoch 10/15\n",
      "80/80 [==============================] - 110s 1s/step - loss: 0.2725 - accuracy: 0.9072 - f1_m: 0.9075 - val_loss: 0.5734 - val_accuracy: 0.7634 - val_f1_m: 0.7750\n",
      "Epoch 11/15\n",
      "80/80 [==============================] - 110s 1s/step - loss: 0.2558 - accuracy: 0.9125 - f1_m: 0.9122 - val_loss: 0.5955 - val_accuracy: 0.7687 - val_f1_m: 0.7800\n",
      "Epoch 12/15\n",
      "80/80 [==============================] - 110s 1s/step - loss: 0.2007 - accuracy: 0.9352 - f1_m: 0.9352 - val_loss: 0.6412 - val_accuracy: 0.7455 - val_f1_m: 0.7580\n",
      "Epoch 13/15\n",
      "80/80 [==============================] - 109s 1s/step - loss: 0.1787 - accuracy: 0.9437 - f1_m: 0.9440 - val_loss: 0.7047 - val_accuracy: 0.7403 - val_f1_m: 0.7530\n",
      "Epoch 14/15\n",
      "80/80 [==============================] - 109s 1s/step - loss: 0.1709 - accuracy: 0.9470 - f1_m: 0.9471 - val_loss: 0.6672 - val_accuracy: 0.7476 - val_f1_m: 0.7600\n",
      "Epoch 15/15\n",
      "80/80 [==============================] - 110s 1s/step - loss: 0.1354 - accuracy: 0.9590 - f1_m: 0.9591 - val_loss: 0.7164 - val_accuracy: 0.7645 - val_f1_m: 0.7760\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "# Get the model object\n",
    "history = model.fit(train_x, train_y, epochs = num_epochs, batch_size = 50, validation_data = (val_x, val_y),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzERVOHcehW5",
    "outputId": "b806ae76-18be-412f-e314-ac94fff6e13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79       473\n",
      "           1       0.82      0.72      0.77       478\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       951\n",
      "   macro avg       0.79      0.78      0.78       951\n",
      "weighted avg       0.79      0.78      0.78       951\n",
      " samples avg       0.78      0.78      0.78       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model.load_weights(model_save_path)\n",
    "preds = model.predict(val_x)>0.5\n",
    "print(classification_report(val_y, preds))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hate Speech Detection MuRIL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
